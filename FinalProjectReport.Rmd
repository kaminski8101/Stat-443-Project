---
title: |
  | \vspace{7cm} \LARGE{Gas Turbine CO Emission Analysis}
author: "Aayushi Gupta, Kyle Kaminski, Rosa Lin, Ruben Martinez"
date: "Client: Darren Glosemeyer"
output: 
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include = FALSE}
# Load required packages
library(readr) 
library(caret)
library(tidyverse)
library(tree)
library(knitr)
library(faraway)

# Create RMSE function
RMSE <- function(y, y_hat) {
  rmse <- sqrt(sum(((y_hat - y)^2)/length(y)))
  print(rmse)
}
```

\newpage

\tableofcontents

\newpage

# Introduction 

The combined cycle power plant, also known as combined cycle gas turbine plant, is an assembly of heat engines that combine to generate electricity (Tüfekci). A combined-cycle power plant (CCPP) is made up of gas turbines, steam turbines, and heat recovery steam generators. The electricity is generated and combined in one cycle by gas and steam turbines and then transferred from one turbine to another.  

We are interested in identifying the process variables that impact carbon monoxide emissions. By determining the process variables that impact carbon monoxide emissions, we will be able to find opportunities to reduce carbon monoxide emissions. 

Our plan is to analyze a dataset that contains 7384 instances of 11 sensor measures that have been aggregated over one hour (by means of average or sum) from a gas turbine located in Turkey for the purpose of studying flue gas emissions, namely CO and NOx (NO and NO2). The data comes from the same power plant as the dataset used for predicting hourly net energy yield. By contrast, this data is collected in another data range (01.01.2011 - 31.12.2015), includes gas turbine parameters (such as Turbine Inlet Temperature and Compressor Discharge pressure) in addition to the ambient variables. Note that the dates are not given in the instances but the data are sorted in chronological order. See the attribute information and [**relevant paper**](https://journals.tubitak.gov.tr/elektrik/issues/elk-19-27-6/elk-27-6-54-1807-87.pdf) for details. Kindly follow the protocol mentioned in the paper (using the first three years' data for training/ cross-validation and the last two for testing) for reproducibility and comparability of works. The dataset can be well used for predicting turbine energy yield (TEY) using ambient variables as features.

## Goal 

The goal for this project is to utilize this data set for the purpose of studying flue gas emissions, specifically carbon monoxide(CO) and nitrogen oxides (NOx). However, our client did tell us to not consider nitrogen oxide, so we will only be focusing on carbon monoxide in this report. Our focus will be to find statistically significant relationships between the ambient, turbine, and emissions variables. We will limit the size of our model to more clearly demonstrate these relationships. Ultimately, we will suggest which variables make the biggest impact on emission levels in order to decrease emissions overall. 

## Gas Turbine CO and NOx Emission Data Set

The data comes from a gas turbine located in Turkey that studies the flue gas emissions of specifically carbon monoxide (CO) and nitrogen oxide (NOx) gases. The data set provides hourly statistics of 11 sensors. Data points were collected from a gas turbine from Jan 01 2011 to Dec 13 2015. 

### Description 

The data file `gt_2015.csv` has 7384 observations and 11 variables from the [**UCI Gas Turbine CO and NOx Emission Data Set**](https://archive.ics.uci.edu/ml/datasets/Gas+Turbine+CO+and+NOx+Emission+Data+Set). We are going to explore and analyze the following variables (more details in Appendices 1): 

* AT - Ambient Temperature
* AP - Ambient Pressure
* AH -  Ambient Humidity
* AFDP - Air filter difference pressure 
* GTEP - Gas turbine exhaust pressure
* TIT - Turbine inlet temperature
* TAT - Turbine after temperature 
* TEY - Turbine energy yield 
* CDP - Compressor discharge pressure
* CO - Carbon Monoxide
* NOX - Nitrogen Oxide (Removed from data)

\newpage

Here’s a quick peek at the data set:

```{r, message=FALSE, echo=FALSE, warning=FALSE}
gt_2015 <- read_csv("Project Data/gt_2015.csv")

# Fix or remove problematic observations (this will be explained later)
gt_2015[c(1363,1364,1585,3977,3978,4762,5752,5753,6901),10] <- gt_2015[c(1363,1364,1585,3977,3978,4762,5752,5753,6901),10] / 10
gt_2015 <- gt_2015[-c(1796,1713,1712,1711,1710,1709,1301,1009,626,121,120,119,118,117,116,115),]
gt_2015_typical <- gt_2015[gt_2015$TEY <= 134.5 & gt_2015$TEY >= 127,]
gt_2015_high <- gt_2015[gt_2015$TEY >= 160,]

# Display table
knitr::kable(head(gt_2015)[,])
```

\newpage

# Methods

```{r include=FALSE}
set.seed(443)
# Set 5-fold cross validation
cv_5 <- trainControl(method = "cv", number = 5)

train_all <- gt_2015 %>% dplyr::select(-NOX) %>% sample_frac(0.8)
test_all <- gt_2015 %>% dplyr::select(-NOX) %>% setdiff(train_all)
train_typical <- gt_2015_typical %>% dplyr::select(-NOX) %>% sample_frac(0.8)
test_typical <- gt_2015_typical %>% dplyr::select(-NOX) %>% setdiff(train_typical)
train_high <- gt_2015_high %>% dplyr::select(-NOX) %>% sample_frac(0.8)
test_high <- gt_2015_high %>% dplyr::select(-NOX) %>% setdiff(train_high)

# AIC stepwise selected linear models 
all_linear_mod <- train(
          form = CO ~ . - TIT - CDP - TEY ,
          data = train_all,
          method = "lmStepAIC",
          trControl = cv_5,
          trace = FALSE
)
typical_linear_mod <- train(
          form = CO ~ . - TIT - AT,
          data = train_typical,
          method = "lmStepAIC",
          trControl = cv_5,
          trace = FALSE
)
high_linear_mod <- train(
          form = CO ~ . - TEY - CDP,
          data = train_high,
          method = "lmStepAIC",
          trControl = cv_5,
          trace = FALSE
)
all_linear_mod_lm <- lm(CO ~ AT + AH + AFDP + GTEP + TAT, data = test_all)
typical_linear_mod_lm <- lm(CO ~ AP + AH + AFDP + GTEP + TAT + TEY + CDP, data = test_typical)
high_linear_mod_lm <- lm(CO ~ AT + AFDP + GTEP + TIT + TAT, data =  test_high)
linear_pred_all <- predict(all_linear_mod_lm, test_all)
linear_pred_typical <- predict(typical_linear_mod_lm, test_typical)
linear_pred_high <- predict(high_linear_mod_lm, test_high)

# Lasso models
all_lasso_mod <- train(
          form = CO ~ . - TIT - CDP - TEY ,
          data = train_all,
          method = "lasso",
          trControl = cv_5
)
typical_lasso_mod <- train(
          form = CO ~ . - TIT - AT,
          data = train_typical,
          method = "lasso",
          trControl = cv_5
)
high_lasso_mod <- train(
          form = CO ~ . - TEY - CDP,
          data = train_high,
          method = "lasso",
          trControl = cv_5
)
lasso_pred_all <- predict(all_lasso_mod, test_all)
lasso_pred_typical <- predict(typical_lasso_mod, test_typical)
lasso_pred_high <- predict(high_lasso_mod, test_high)

# Decision Trees
tree_CO_all <- tree(CO ~ . , train_all, 
                  control = tree.control(nobs = length(train_all$CO), 
                                         minsize = 4, mindev=0.001), method = "recursive.partition")
tree_CO_typical <- tree(CO ~ . , train_typical,
                        control = tree.control(nobs = length(train_typical$CO), 
                                         minsize = 4, mindev=0.001), method = "recursive.partition")
tree_CO_high <- tree(CO ~ . , train_high,
                     control = tree.control(nobs = length(train_high$CO), 
                                         minsize = 4, mindev=0.001), method = "recursive.partition")
pruned_tree_all <- prune.tree(tree_CO_all, best = 8)
pruned_tree_typical <- prune.tree(tree_CO_typical, best = 7)
pruned_tree_high <- prune.tree(tree_CO_high, best = 6)
tree_pred_all <- predict(pruned_tree_all, test_all)
tree_pred_typical <- predict(pruned_tree_typical, test_typical)
tree_pred_high <- predict(pruned_tree_high, test_high)
```



## Exploratory Data Analysis 

### Pairwise Correlations 

&nbsp;
&nbsp;

```{r, message=FALSE, echo=FALSE, warning=FALSE}
pairs(gt_2015, pch = 20, cex = 0.25, main = "Figure 1: Pairwise Correlation Plot")
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
knitr::kable(cor(gt_2015), digits = 2, caption = "Pairwise Correlation Between Variables")
```

The exploratory analysis shows possible linear relationships between the response variable CO and the feature variables CDP, TEY, TIT, GTEP and AFDP. The analysis also indicates possible collinearity between some of the feature variables (TIT, CDP, and TEY). This could cause some problems in our analysis and will likely lead to the removal of the redundant variables. 


### Carbon Monoxide Distribution

The client provided us a set of production ranges to analyze. An overall production range that analyzes all of the data points from the carbon monoxide emission output, a typical production range which looks at data points from 130 to 136, and a high production range that looks at data points higher than 160. 

&nbsp;
&nbsp;

```{r, message=FALSE, echo=FALSE, warning=FALSE}
d = density(gt_2015$TEY)
plot(d, xlab = "Turbine Energy Yield", ylab = "Density", main = "Figure 2: Turbine Energy Yield Distribution")
polygon(d, col = "blue4", border = "chocolate1")
abline(v = c(127,134.5,160), lty = c(1,1,2), col = c("red","red","black"))
```

The typical production range the client provided did not fully capture the typical production range that we observed in our data sample (see Figure 2 above). This could be a result of the data values from the 2015 data set having lower values compared to other data sets. Therefore, we decided to shift the typical production range to 127 to 134.5 given that it is a better representation of the typical production range of the carbon monoxide emission output. 


### Data Preparation 

The first step to preparing the data was to remove the response variable nitrogen oxide, since our analysis solely focuses on carbon monoxide emissions. 

Since we were able to anticipate variables that could cause some problems in our linear based analyses due to collinearity, we decided to remove the following variables from our linear based models: 

* TIT
* CDP 
* TEY 


## Model Selection 

To accurately identify the process variables that impact carbon monoxide emissions, we decided to examine three different models to make sure that the model we selected was the most useful and effective way of analyzing the data set. The three models we used were **Multiple Linear Regression**, **Lasso**, and **Decision Trees**. 

### RMSE

In order to determine which model was the most effective, we compared the RMSE of multiple linear regression, lasso, and decision tree models. Root Mean Squared Errors are the standard deviation of residuals. The point of calculating the RMSE is to measure how spread out these variables are. The rule of thumb is, the lower the RMSE, the better. 

### Training and Testing Data

For all of our models, we split our data into training and testing datasets to avoid overfitting the models. By doing so, we minimized the effects of data discrepancies and effectively evaluated our models. 


\newpage 

# Results

## Decision Tree Model Selection 

In the table below, the decision tree outperforms linear regression and lasso in the overall production range, typical production range, and high production range. Therefore, we decided to use the Decision Tree Model to examine the biggest impact on emission levels in order to decrease emissions overall. 

```{r,include = FALSE,  message=FALSE, echo=FALSE, warning=FALSE}
all_linear_rmse     <- RMSE(test_all$CO, linear_pred_all)
typical_linear_rmse <- RMSE(test_typical$CO, linear_pred_typical)
high_linear_rmse    <- RMSE(test_high$CO, linear_pred_high)

all_lass_rmse     <- RMSE(test_all$CO, lasso_pred_all)
typical_lass_rmse <- RMSE(test_typical$CO, lasso_pred_typical)
high_lass_rmse    <- RMSE(test_high$CO, lasso_pred_high)

all_tree_rmse     <- RMSE(test_all$CO, tree_pred_all)
typical_tree_rmse <- RMSE(test_typical$CO, tree_pred_typical)
high_tree_rmse    <- RMSE(test_high$CO, tree_pred_high)
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
RMSE_Table <- matrix(c(round(all_linear_rmse    ,digits = 4),
                       round(typical_linear_rmse,digits = 4),
                       round(high_linear_rmse   ,digits = 4),
                       round(all_lass_rmse      ,digits = 4),
                       round(typical_lass_rmse  ,digits = 4),
                       round(high_lass_rmse     ,digits = 4),
                       round(all_tree_rmse      ,digits = 4),
                       round(typical_tree_rmse  ,digits = 4),
                       round(high_tree_rmse     ,digits = 4 )) 
                     ,ncol=3, byrow=TRUE)
colnames(RMSE_Table) <- c("Overall Production Range", "Typical Production Range (127-134.5)", "High Production Range (160+)")
rownames(RMSE_Table) <- c("Linear Regression", "Lasso", "Decision Tree" )
RMSE_Table <- as.table(RMSE_Table)
RMSE_Table <- kable(RMSE_Table)
RMSE_Table
```

## Overall Decision Tree Model

\begin{figure}[H]
\centering
\includegraphics{./images/overall_tree.png}
\caption{Overall Production Range Decision Tree}
\end{figure}

The decision tree above represents the final tree model that was trained on the entire data set supplied to us. The first split the tree made was on the turbine inlet temperatures, separating observations where the TIT was less than 1049.75 to the left and the remaining observations to the right. If we observe all of the terminal nodes on each side of the tree after this first split, it is clear that the higher TIT values resulted in lower CO values. Similar to the TIT values, it is also observed that **higher TAT, AT, and AFDP** values also resulted in lower CO output as well. 


## Typical Decision Tree Model 

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./images/typical_tree.png}
\caption{Typical Production Range Decision Tree}
\end{figure}

This decision tree represents our final tree model that was trained on the typical energy production range with TEY values between 127 and 134.5. This tree first split on AT, and actually terminates when the AT is greater than 11.9655. We can infer AT is likely the most important variable in this energy production range with the higher AT values resulting in lower CO, agreeing with our overall tree model. We can conclude **Lower GTEP and higher AT** values look to result in lowering CO output.


## High Decision Tree Model

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{./images/high_tree.png}
\caption{High Production Range Decision Tree}
\end{figure}

This decision tree represents our final tree model that was built on the high production range data with TEY values over 160. This tree argues that higher AFDP values on average result in lower CO output because the average value of the nodes on the right side is lower than those on the left. Unlike our previous models, AT does not show a very strong relationship with the CO output values. **Higher TEY and lower TAT** values look to have lower CO outputs. 

## Tree Model Explanations 

### Carbon Monoxide Correlations

**----- ADD TABLE 6 HERE -----**

The table above shows us the single correlations between CO and the explanatory variables that were used in the tree model splits. We found it surprising that high TIT, TAT, and AT resulted in lower CO outputs as we would think that cooler temperatures would result in more efficient energy production. The correlation table does show us that TIT and AT both have negative correlations with CO, meaning as one increases the other decreases. This supports the arguments made in our tree models. TAT has an almost 0 correlation with CO according to the data, which does not agree or disagree with our models since the TAT splits were always after other splits.

### Ambient Temperature Plots

```{r,  message=FALSE, echo=FALSE, warning=FALSE}
model = lm(gt_2015$AT ~ gt_2015$TEY)
ggplot(model, aes(x = gt_2015$TEY, y = gt_2015$AT), xlim = c(100,180)) + geom_jitter(width = 3, color = "navy") + theme_classic() + labs(x = "Total Energy Yield") + labs(y = "Ambient Temperature") + labs(title = "Figure 6: Relationship between Total Energy Yield and Ambient Temperature")
```

```{r,  message=FALSE, echo=FALSE, warning=FALSE}
model2 = lm(gt_2015$AT ~ gt_2015$CO)
ggplot(model, aes(x = gt_2015$CO, y = gt_2015$AT), xlim = c(100,180)) + geom_jitter(width = 3, color = "navy") + theme_classic() + labs(x = "Carbon Monoxide") + labs(y = "Ambient Temperature") + labs(title = "Figure 7: Relationship between Carbon Monoxide and Ambient Temperature")
```


Figures 6 and 7 above help further explain the why higher AT result in lower CO outputs. Figure 6 shows no clear trend with TEY values less than around 145, but a clear negative trend when the TEY values are above 145. This shows the extreme TEY values are impacted by AT, likely because at very high temperatures the machines can not work as hard or they will overheat. Figure 7 on the bottom shows AT against CO, where we see 9 CO values greater than 17 whereas the other 7300 observations are less. 

\newpage

# Conclusion 

## Most Sensitive Process Variables 

Based on our results, the following variables are the most sensitive process variables for the overall production range:

* NA
* NA

Based on our results, the following variables are the most sensitive process variables for the typical production range (127 - 134.5):

* NA
* NA

Based on our results, the following variables are the most sensitive process variables for the high production range (160+):

* NA
* NA

As you can see...

## Suggestions 



\newpage

# Appendix

## Multiple Linear Regression

We will create a multiple linear regression model using the feature variables remaining after preparing our data -- AT, AP, AH, AFDP, GTEP, and TAT. The implementation and parameters of this model can be obtained by the following equation where we will find estimates for the parameters $\beta$ using:

$$\hat{\beta} = (X^TX)^{-1}X$$
[[**Source**]](https://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html)

Key assumptions are stated as: 

* **L**inearity: can be written as a linear combination of the predictors.
* **I**ndependence: the errors are independent of each other (not highly correlated).
* **N**ormality: the distribution of the errors follow a normal distribution.
* **E**qual Variance: the error variance is the same. 

We will then use model selection using VIF to tune our model and remove any insignificant predictor variables. This selection prefers smaller models which aligns with our goal of limiting the size of our final model.


## Lasso 

The Lasso model is similar in structure to the linear model, but it differs in how the variable selection process is treated. Lasso models often perform better than a simple/multiple linear regression because the Lasso model can penalize unimportant variables by shrinking their corresponding coefficients, which decreases the influence those variables have on the model. This is preferable over the linear regression model because the variance can be decreased without largely impacting the model’s bias.

$$\sum_{i=1}^{n} \Big(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\Big)^2 + \lambda \sum_{j=1}^{p} |\beta_j|$$

## Variance Inflation Factor (VIF)

Variance Inflation Factor detects multicollinearity in regression analysis. Multicollinearity is when the correlation between predictors affects regression results. We only used VIF in our linear based models.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
full_model_all     = lm(CO ~ . - NOX, data = gt_2015)
full_model_typical = lm(CO ~ . - NOX, data = gt_2015_typical)
full_model_high    = lm(CO ~ . - NOX, data = gt_2015_high)

invisible(vif(full_model_all    ))
invisible(vif(full_model_typical))
invisible(vif(full_model_high   ))

second_model_all     = lm(CO ~ . - NOX - TIT, data = gt_2015)
second_model_typical = lm(CO ~ . - NOX - TIT, data = gt_2015_typical)
second_model_high    = lm(CO ~ . - NOX - TEY, data = gt_2015_high)

invisible(vif(second_model_all    ))
invisible(vif(second_model_typical))
invisible(vif(second_model_high   ))

third_model_all     = lm(CO ~ . - NOX - TIT - CDP, data = gt_2015)
third_model_typical = lm(CO ~ . - NOX - TIT - AT, data = gt_2015_typical)
third_model_high    = lm(CO ~ . - NOX - TEY - CDP, data = gt_2015_high)

invisible(vif(third_model_all    ))
invisible(vif(third_model_typical))
invisible(vif(third_model_high   ))

initial_typical_linear_model <- third_model_typical
initial_high_linear_model    <- third_model_high

fourth_model_all     = lm(CO ~ . - NOX - TIT - CDP - TEY, data = gt_2015)

invisible(vif(fourth_model_all))

initial_all_linear_model <- fourth_model_all
```

**VIF for overall production range** 

```{r, message=FALSE, echo=FALSE, warning=FALSE}
vif(initial_all_linear_model)
```

**VIF for typical production range** 

```{r, message=FALSE, echo=FALSE, warning=FALSE}
vif(initial_typical_linear_model)
```

**VIF for high production range**

```{r, message=FALSE, echo=FALSE, warning=FALSE}
vif(initial_high_linear_model)
```




[[**Source**]](https://statisticallearning.org/regularization.html)


## Decision Tree 

Decision trees are nonparametric models and work by taking in all of the characteristics of the observations, and then splitting the data into separate groups based on the optimal splitting characteristics. These models are called decision tree models because each split can be thought of as a branch in a tree. The leaves are thus called terminal nodes in this model because that is where the model outputs the prediction based on all the splitting criteria up until that point. A decision tree can be used to predict both categorical outcomes and quantitative outcomes. In this analysis, we are looking for a numeric outcome so a regression tree is used. 


$$\textrm{Gini}(K) = \sum_{i \in N} P_{i,K}(1-P_{i,K}) = 1 - \sum_{i \in N} P^2_{i,K}$$


[[**Source**]]()

## Correlations

Correlation is a statistical measure that measures which two variables are linearly related. It is commonly used to describe simple relationships without discussing the cause and effect.

$$r = \frac{\sum(x-m_x)(y-m_y)}{\sqrt{\sum(x-m_x)^2\sum(y-m_y)^2}}$$

[[**Source**]](https://www.geeksforgeeks.org/python-pearson-correlation-test-between-two-variables/)


Individual Contributions:

Aayushi:

Kyle:

Rosa:

Ruben
